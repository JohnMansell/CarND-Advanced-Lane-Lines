{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding\n",
    ">### John Mansell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "> The goal of this project is to take a video of a road, identify the lane lines, and plot those lines back onto the original video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warping the Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Camera Calibration\n",
    "> Camera lenses tend to distort the true image, espcially near the edges and corners. If the starting image is not an acruate picture of the road, then the conclusions will be off. \n",
    "\n",
    "> One powerful technique for detecting and correcting the distortion of a camera's lens is to take pictures of lines known to be straight (such as a picture of a chessboard pattern). Once the distortion is measured, then a matrix can be generated which will transform images from 'distortion space' into 'undistorted space'.\n",
    "\n",
    "> Below is an example of the warping that happens when an image is undistored with this technique.\n",
    "![](output_images/Undistorted.png)\n",
    "\n",
    "> ## Perspective Transform\n",
    "> Straight lines are easy enough to find from a picture of a road, but curved lanes are curving in the plane of the road, and the perspective is at an angle to that plane. To make it easier to calculate the curvature of the lane, a linear transform was applied to the images. This had the effect of shifting the perspective to a top-down view of the road.\n",
    "![](output_images/PerspectiveTransform.png)\n",
    "\n",
    "> The lane is now in a 2D plane, which makes the curvature much easier to calculate.\n",
    "\n",
    "> To ensure a good fit, I warped an image of straight lane lines and compared the result to red calibration lines which are perfectly vertical.\n",
    "![](output_images/WarpCalibration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolating the Lane Lines\n",
    "> The next step was to apply various filters to the warped image which isolate the lane line pixels.\n",
    "\n",
    "> ### Color\n",
    "> Converting the images into different colorspaces is a convenient way to make the lane lines stand out even in variable lighting conditions.\n",
    "![](output_images/Color_Channels.png)\n",
    "> ### Gradient\n",
    "> Annother useful technique is to scan the image for gradients, or dramatic changes in pixel value. Gradients can be filtered by their direction, magnitude, or both. I used a combination of magnitude and directional filtering. I used the magnitude to filter out noise, and used the direction to select for gradients which were going in aproximately the direction of the lane lines.\n",
    "\n",
    ">![](output_images/GradientThresholds.png)\n",
    "\n",
    "> ### Creating a binary image\n",
    "> Finally, all the processes are tied together. The goal is to create a robust pipeline which will identify lane line pixels in a variety of lighting and road conditions. I used jupyter notebook widgets to test the effects of tuning the myriad of different parameters.\n",
    "\n",
    "> The final output goal is a binary image. Lane line pixels are 1, and all other pixels are 0\n",
    "\n",
    "> For the specific parameters used, see \"Hyper Parameters\" -- Helper Functions(16 - 29)\n",
    "![](output_images/BinaryOutput.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the Lines\n",
    "> Once a binary image has been created, the next goal is to map the position and curvature of the lane.\n",
    "\n",
    "> ### Pattern fitting straight lines\n",
    "> For me, the first test when looking for lane lines was to check for straight lane lines. If straight lane lines were found, the other tests were skipped. \n",
    "> 1. Create a template of straight lane lines\n",
    "![](output_images/StraightLineTemplate.png)\n",
    "> 2. Compare the binary image to the template. This is a weighted sum of the images. Pixels in the binary image inside the lane lines are multiplied by 1. Pixels in the binary image outside the lane lines are multiplied by -1.  \n",
    "```test = np.zeros_like(binary)                \n",
    "test[(s_template == 1) & (binary == 1)] = 1  \n",
    "test[(s_template == 0) & (binary == 1)] = -1   ```\n",
    "![](output_images/Straight_vs_template.png)\n",
    "> *These images are colored to show the effect, for the code, see **\"Straight Line Template\"** and **\"Pattern Fit\"** in [Helper Functions](./helper_functions.py) (420 - 457) & (461 - 482)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Testing for Curves\n",
    "> If the lane lines don't fit inside the straight line template, then the image is searched for the specific location and curvature of the lane line. The first step is to find the base of the lane line. To find the base of the lane line, we sum the bottom 4th of the image, and map that to a 1d axis.\n",
    "![](output_images/histogram.jpg)\n",
    "> The two strongest peaks, in the general region where we expect to find lane lines, are set as the base of the lane lines.\n",
    "\n",
    "\n",
    "> Once the base is found, we incrimentally search the image just above the most recent section of the lane line, moving up the image, as we map out the location of the lane line within the frame.\n",
    ">![](output_images/WindowTiles.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the Lanes\n",
    "\n",
    "> ### Polynomial Lanes\n",
    "> For each window above, the center is added into an array. This array is then fed into the function np.polyfit() which finds the best fit polynomial for the input. This polynomial represents the lane line as a function in the (x,y) plane of the road.\n",
    "\n",
    "> The polynomial for each lane is then drawn onto a blank image.\n",
    "![](output_images/LaneLinesDrawn.png)\n",
    "\n",
    "> ### Unwarp\n",
    "> Next, we draw the lane lines onto the original image. We undo the perspective transform we did to the image origninally. The 'unwarped' lane lines are then drawn onto the original image.\n",
    "![](output_images/Result_1.png)\n",
    "\n",
    ">### Math parts\n",
    "> The last step in the image analysis is to calculate the radius of curvature of the road and the cars position relative to the center or the lane. \n",
    "![](output_images/FinalResult.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video\n",
    "> Finally, we repeat the entire process for each frame in a video and output a video which has the lane-lines, the curvature, and the distance from center mapped onto the video.\n",
    "\n",
    "> I chose to also include a picture in picture image showing the binary image created and the lane line windows found in that image.\n",
    "\n",
    "> **See output_videos / output_project_video.mp4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulties\n",
    "> The pipeline that I created had difficulty in the extreme lighting conditions, especially really bright conditions where the lane line at a far distance blended into the road. \n",
    "\n",
    "# Next Steps\n",
    "> The next steps would be to create a pipeline which is more robust and can handle more road / lighting conditions. There were also some assumptions I was able to make because of the nature of the project video that would need to be dealt with to create a pipeline that could work in the real world\n",
    "> * **Lane width** -- I made the assumption that the lane width would stay constant which is not always true in real world conditions. This allowed me to test for the strongest two signals separated by the spread (the distance between lane lines). In many conditions, (merging, splitting, construction, different lane width standards, etc) this would not be a safe assumption to make.\n",
    "> * **Lane Line Base** -- I made the assumption that the base of the lane line within the frame would be within a narrow window. This was an effective way to rule out false positive signals from other strong lines such as the concrete barrier along the side of the road. However, this would not always be true in real world conditions, especially changing lanes or on a really curvy road such as in the Harder Challenge Video.\n",
    "> * **Pattern Fitting** -- I successfully tested for straight lane lines agains a template, I'd like to also add in templates for common curvatures so that those could be tested without the whole curve finding pipeline. I believe this would result in a quicker, more accurate pipeline.\n",
    "> * **Empty Lane** -- In this project video, the lane in front of the car was empty, which allowed me to simplify the pipeline. If this had been taken in bumper to bumper traffic, the pipeline would need to be refined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements:\n",
    "> **Udacity Lesson:**  \n",
    "> Many of the functions were taken directly from the Udacity Self Driving Car lessons and then modified as needed for the project.  \n",
    "> **Udacity Q/A video:**  \n",
    "> I started with the Q&A on youtube for this project and followed the walk through. The video did an excellent job bringing all the lessons together. I used the project built by following the video as the starting point to start the project.  \n",
    "> **Other Students:**  \n",
    "> I looked at other students submissions for ideas on what had worked well for them, specifically, [jeremy_shannon's](https://github.com/jeremy-shannon/CarND-Advanced-Lane-Lines) submission was what tipped me off to the value of using the LAB and HSV color spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
